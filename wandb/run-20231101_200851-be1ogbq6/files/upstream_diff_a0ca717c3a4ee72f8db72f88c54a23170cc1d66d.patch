diff --git a/requirements.txt b/requirements.txt
index 62018da..afd62ef 100644
--- a/requirements.txt
+++ b/requirements.txt
@@ -18,3 +18,4 @@ torchaudio==0.13.1
 torchvision==0.14.1
 tqdm==4.65.0
 gym==0.21.0
+wandb==0.13.11
diff --git a/src/config/algs/icq.yaml b/src/config/algs/icq.yaml
index a046dff..75e69aa 100644
--- a/src/config/algs/icq.yaml
+++ b/src/config/algs/icq.yaml
@@ -6,11 +6,12 @@ epsilon_finish: .05
 epsilon_anneal_time: 500000
 mask_before_softmax: False
 
-runner: "episode"
+#runner: "episode"
+runner: "parallel"
 
 buffer_size: 32
-off_buffer_size: 70000 # size of the off-policy buffer default: 5000
-batch_size_run: 1 # 10
+# off_buffer_size : 70000 # size of the off-policy buffer default: 5000
+batch_size_run: 10 # 10
 batch_size: 16
 
 
diff --git a/src/config/algs/iql_beta.yaml b/src/config/algs/iql_beta.yaml
deleted file mode 100644
index 8df0542..0000000
--- a/src/config/algs/iql_beta.yaml
+++ /dev/null
@@ -1,23 +0,0 @@
-# --- IQL specific parameters ---
-
-# use epsilon greedy action selector
-action_selector: "epsilon_greedy"
-epsilon_start: 1.0
-epsilon_finish: 0.05
-epsilon_anneal_time: 50000
-
-runner: "parallel"
-batch_size_run: 8
-
-buffer_size: 5000
-
-# update the target network every {} episodes
-target_update_interval_or_tau: 200
-
-# use the Q_Learner to train
-agent_output_type: "q"
-learner: "q_learner"
-double_q: True
-mixer: # Mixer becomes None
-
-name: "iql_smac_parallel"
\ No newline at end of file
diff --git a/src/config/algs/itd3_cql.yaml b/src/config/algs/itd3_cql.yaml
deleted file mode 100644
index 24b4b43..0000000
--- a/src/config/algs/itd3_cql.yaml
+++ /dev/null
@@ -1,35 +0,0 @@
-# --- ITD3+CQL specific parameters ---
-
-runner: "episode"
-
-buffer_size: 50000
-# update the target network every {} episodes
-target_update_interval_or_tau: .005
-
-obs_agent_id: True
-obs_last_action: False
-obs_individual_obs: True
-
-
-# use the madddpg_learner to train
-mac: "maddpg_mac"
-reg: 0.001
-batch_size: 32
-lr: 0.0003
-use_rnn: False # better withoyt rnn (at least on expert data)
-
-standardise_returns: False
-standardise_rewards: True
-
-learner: "itd3_learner"
-agent_output_type: "pi_logits"
-hidden_dim: 128
-critic_type: "itd3_critic"
-
-optim_type: "adam"
-actor_freq: 2
-cql_alpha: 1.0
-critic_rnn: False
-
-name: "itd3_cql"
-
diff --git a/src/config/algs/matd3_cql.yaml b/src/config/algs/matd3_cql.yaml
deleted file mode 100644
index 4d14d4c..0000000
--- a/src/config/algs/matd3_cql.yaml
+++ /dev/null
@@ -1,40 +0,0 @@
-# --- MATD3+CQL specific parameters ---
-
-runner: "episode"
-
-buffer_size: 50000
-# update the target network every {} episodes
-target_update_interval_or_tau: .005
-
-obs_agent_id: True
-obs_last_action: False
-obs_individual_obs: False
-
-critic_individual_obs: True
-critic_last_action: False
-critic_agent_id: True
-
-# use the madddpg_learner to train
-mac: "maddpg_mac"
-reg: 0.001
-batch_size: 32
-lr: 0.0003
-use_rnn: True
-
-standardise_returns: False
-standardise_rewards: True
-
-learner: "matd3_learner"
-agent_output_type: "pi_logits"
-hidden_dim: 128
-critic_type: "maddpg_critic"
-
-optim_type: "adam"
-actor_freq: 2
-
-cql_alpha: 1.0
-cql_temperature: 1.0
-num_repeats: 10
-
-name: "matd3_cql"
-
diff --git a/src/config/algs/omar.yaml b/src/config/algs/omar.yaml
index cd300dc..9f7f4db 100644
--- a/src/config/algs/omar.yaml
+++ b/src/config/algs/omar.yaml
@@ -1,39 +1,40 @@
 # --- OMAR specific parameters ---
 
-runner: "episode"
+action_selector: "multinomial"
+epsilon_start: .5
+epsilon_finish: .05
+epsilon_anneal_time: 500000
+mask_before_softmax: True
 
-buffer_size: 50000
-# update the target network every {} episodes
-target_update_interval_or_tau: .005
+runner: "parallel"
 
-obs_agent_id: True
-obs_last_action: False
-obs_individual_obs: True
+batch_size_run: 8 # batch_size_run=4, buffer_size = 2500, batch_size=64  for 3s5z_vs_3s6z
 
+buffer_size: 32
+batch_size: 128
 
-# use the madddpg_learner to train
-mac: "maddpg_mac"
-reg: 0.001
-batch_size: 32
-lr: 0.0003
-use_rnn: True
+# env_args:
+#   state_last_action: False # critic adds last action internally
 
-standardise_returns: False
-standardise_rewards: True
+# update the target network every {} training steps
+target_update_interval: 200
 
-learner: "itd3_learner"
-agent_output_type: "pi_logits"
-hidden_dim: 128
-critic_type: "itd3_critic"
 
-optim_type: "adam"
-actor_freq: 2
+lr: 0.001
+critic_lr: 0.001
+td_lambda: 0.6
+cql_alpha: 5.0
+
 
-omar_iters: 3
-omar_num_samples: 10 
-init_omar_mu: 0
-init_omar_sigma: 2.0
-omar_coe: .5
+omar_iters: 5
+omar_coe: 0.5
+omar_num_samples: 10
+# use qmix
+mixing_embed_dim: 32
+
+# use COMA
+agent_output_type: "pi_logits"
+learner: "omar_learner"
 
-name: "omar"
 
+name: "omar"
\ No newline at end of file
diff --git a/src/config/algs/qmix_beta.yaml b/src/config/algs/qmix_beta.yaml
deleted file mode 100644
index e220c3f..0000000
--- a/src/config/algs/qmix_beta.yaml
+++ /dev/null
@@ -1,24 +0,0 @@
-# --- QMIX specific parameters ---
-
-# use epsilon greedy action selector
-action_selector: "epsilon_greedy"
-epsilon_start: 1.0
-epsilon_finish: 0.05
-epsilon_anneal_time: 50000
-
-runner: "parallel"
-batch_size_run: 8
-
-buffer_size: 5000
-
-# update the target network every {} episodes
-target_update_interval_or_tau: 200
-
-# use the Q_Learner to train
-agent_output_type: "q"
-learner: "q_learner"
-double_q: True
-mixer: "qmix"
-mixing_embed_dim: 32
-
-name: "qmix_smac_parallel"
diff --git a/src/config/algs/vdn_beta.yaml b/src/config/algs/vdn_beta.yaml
deleted file mode 100644
index 1443b51..0000000
--- a/src/config/algs/vdn_beta.yaml
+++ /dev/null
@@ -1,23 +0,0 @@
-# --- VDN specific parameters ---
-
-# use epsilon greedy action selector
-action_selector: "epsilon_greedy"
-epsilon_start: 1.0
-epsilon_finish: 0.05
-epsilon_anneal_time: 50000
-
-runner: "parallel"
-batch_size_run: 8
-
-buffer_size: 5000
-
-# update the target network every {} episodes
-target_update_interval_or_tau: 200
-
-# use the Q_Learner to train
-agent_output_type: "q"
-learner: "q_learner"
-double_q: True
-mixer: "vdn"
-
-name: "vdn_smac_parallel"
diff --git a/src/config/default.yaml b/src/config/default.yaml
index 58f7423..4743417 100644
--- a/src/config/default.yaml
+++ b/src/config/default.yaml
@@ -25,6 +25,10 @@ evaluate: False # Evaluate model for test_nepisode episodes and quit (no trainin
 load_step: 0 # Load model trained on this many timesteps (0 if choose max possible)
 save_replay: False # Saving the replay of the model loaded from checkpoint_path
 local_results_path: "results" # Path for local results
+use_wandb: True 
+resume_id: null
+wandb_project_name: "offpymarl" # Name of the wandb project
+
 
 # --- RL hyperparameters ---
 gamma: 0.99
diff --git a/src/config/envs/sc2_decompose.yaml b/src/config/envs/sc2_decompose.yaml
deleted file mode 100644
index 23737b2..0000000
--- a/src/config/envs/sc2_decompose.yaml
+++ /dev/null
@@ -1,52 +0,0 @@
-env: sc2
-
-env_args:
-  continuing_episode: False
-  difficulty: "7"
-  game_version: null
-  map_name: "3m"
-  move_amount: 2
-  obs_all_health: True
-  obs_instead_of_state: False
-  obs_last_action: False
-  obs_own_health: True
-  obs_pathing_grid: False
-  obs_terrain_height: False
-  obs_timestep_number: False
-  reward_death_value: 10
-  reward_defeat: 0
-  reward_negative_scale: 0.5
-  reward_only_positive: True
-  reward_scale: True
-  reward_scale_rate: 20
-  reward_sparse: False
-  reward_win: 200
-  replay_dir: ""
-  replay_prefix: ""
-  state_last_action: True
-  state_timestep_number: False
-  step_mul: 8
-  seed: null
-  heuristic_ai: False
-  heuristic_rest: False
-  debug: False
-
-test_greedy: True
-test_nepisode: 32
-test_interval: 10000
-log_interval: 10000
-runner_log_interval: 10000
-learner_log_interval: 10000
-t_max: 2050000
-
-offline_data_folder: "dataset"
-offline_bottom_data_path: ""
-max_size: 2000
-offline_batch_size: 20
-offline_data_type: "h5"
-offline_data_quality: "expert"
-
-decomposer_checkpoint_path: "decomposers"
-agent_wise_decompose: False
-decompose_type: "bellman"
-normalize_agent_wise_reward: False
\ No newline at end of file
diff --git a/src/learners/itd3_learner.py b/src/learners/itd3_learner.py
index 232c651..365687f 100644
--- a/src/learners/itd3_learner.py
+++ b/src/learners/itd3_learner.py
@@ -67,7 +67,7 @@ class ITD3Learner:
     
 
     def train(self, batch: EpisodeBatch, t_env: int, episode_num: int):
-        critic_log = self.train_critic(batch)
+        critic_log, top_acs = self.train_critic(batch)
         
         if (self.training_steps + 1) % self.actor_freq == 0:
             batch_size = batch.batch_size
@@ -108,52 +108,54 @@ class ITD3Learner:
                 actor_loss = td3_loss + bc_loss
                 
             elif "omar" in self.args.name:
-                raise NotImplementedError("Sth should be corrected with omar in discrete, sigma tends to be zero")
-                # Problem, how to define avail_actions?
-                self.omar_mu = th.zeros((batch_size, batch.max_seq_length-1, self.n_agents, self.n_actions)) + self.init_omar_mu
-                self.omar_sigma = th.zeros((batch_size, batch.max_seq_length-1, self.n_agents, self.n_actions)) + self.init_omar_sigma + 1e-5
-                self.omar_mu = self.omar_mu.to(batch.device)
-                self.omar_sigma = self.omar_sigma.to(batch.device)
-                formatted_critic_inputs = critic_inputs.unsqueeze(0).repeat(self.omar_num_samples, 1, 1, 1, 1).\
-                    view(self.omar_num_samples * batch_size, batch.max_seq_length, self.n_agents, -1)[:, :-1]
-                formatted_avail_actions = avail_actions.unsqueeze(0).repeat(self.omar_num_samples, 1, 1, 1, 1).\
-                    view(self.omar_num_samples * batch_size, -1, self.n_agents, self.n_actions)
-                for iter_idx in range(self.omar_iters):
-                    # print(th.all(self.omar_mu>=0), th.all(self.omar_sigma >= 0))
-                    # assert th.all(self.omar_sigma >= 0)
-                    # self.omar_sigma = th.zeros((batch_size, batch.max_seq_length-1, self.n_agents, self.n_actions)) + self.init_omar_sigma
-                    # self.omar_sigma = self.omar_sigma.to(batch.device)
-                    dist = th.distributions.Normal(self.omar_mu, self.omar_sigma)
-                    cem_sampled_acs = dist.sample((self.omar_num_samples,)).detach() # (samples, bs, T-1, n_agents, n_actions)
-                    cem_sampled_acs = cem_sampled_acs.view(self.omar_num_samples * batch_size, -1, self.n_agents, self.n_actions)
-                    cem_sampled_acs[formatted_avail_actions == 0] = -1e10
-                    discrete_cem_sampled_acs = F.one_hot(cem_sampled_acs.argmax(dim=-1))  # (samples*bs, T-1, n_agents, n_actions)
-                    # print(discrete_cem_sampled_acs.shape, formatted_critic_inputs.shape)
-                    all_pred_qvals = self.critic1(formatted_critic_inputs, discrete_cem_sampled_acs).\
-                        view(self.omar_num_samples, batch_size, -1, self.n_agents, 1)
-                    discrete_cem_sampled_acs = discrete_cem_sampled_acs.\
-                        view(self.omar_num_samples, batch_size, -1, self.n_agents, self.n_actions)
+                raise NotImplementedError("Omar is not implemented yet")
+                # Omar
+                # self.omar_mu = th.zeros((batch_size, batch.max_seq_length-1, self.n_agents, self.n_actions)) + self.init_omar_mu
+                # self.omar_sigma = th.zeros((batch_size, batch.max_seq_length-1, self.n_agents, self.n_actions)) + self.init_omar_sigma + 1e-5
+                # self.omar_mu = self.omar_mu.to(batch.device)
+                # self.omar_sigma = self.omar_sigma.to(batch.device)
+                # formatted_critic_inputs = critic_inputs.unsqueeze(0).repeat(self.omar_num_samples, 1, 1, 1, 1).\
+                #     view(self.omar_num_samples * batch_size, batch.max_seq_length, self.n_agents, -1)[:, :-1]
+                # formatted_avail_actions = avail_actions.unsqueeze(0).repeat(self.omar_num_samples, 1, 1, 1, 1).\
+                #     view(self.omar_num_samples * batch_size, -1, self.n_agents, self.n_actions)
+                # for iter_idx in range(self.omar_iters):
+                #     # print(th.all(self.omar_mu>=0), th.all(self.omar_sigma >= 0))
+                #     # assert th.all(self.omar_sigma >= 0)
+                #     # self.omar_sigma = th.zeros((batch_size, batch.max_seq_length-1, self.n_agents, self.n_actions)) + self.init_omar_sigma
+                #     # self.omar_sigma = self.omar_sigma.to(batch.device)
+                #     dist = th.distributions.Normal(self.omar_mu, self.omar_sigma)
+                #     cem_sampled_acs = dist.sample((self.omar_num_samples,)).detach() # (samples, bs, T-1, n_agents, n_actions)
+                #     cem_sampled_acs = cem_sampled_acs.view(self.omar_num_samples * batch_size, -1, self.n_agents, self.n_actions)
+                #     cem_sampled_acs[formatted_avail_actions == 0] = -1e10
+                #     discrete_cem_sampled_acs = F.one_hot(cem_sampled_acs.argmax(dim=-1))  # (samples*bs, T-1, n_agents, n_actions)
+                #     # print(discrete_cem_sampled_acs.shape, formatted_critic_inputs.shape)
+                #     all_pred_qvals = self.critic1(formatted_critic_inputs, discrete_cem_sampled_acs).\
+                #         view(self.omar_num_samples, batch_size, -1, self.n_agents, 1)
+                #     discrete_cem_sampled_acs = discrete_cem_sampled_acs.\
+                #         view(self.omar_num_samples, batch_size, -1, self.n_agents, self.n_actions)
             
-                    # self.omar_mu = self._compute_softmax_acs(all_pred_qvals, discrete_cem_sampled_acs)
-                    cem_sampled_acs = cem_sampled_acs.view(self.omar_num_samples, batch_size, -1, self.n_agents, self.n_actions)
-                    self.omar_mu = self._compute_softmax_acs(all_pred_qvals, cem_sampled_acs)
-                    # self.omar_sigma  = th.sqrt(th.mean((discrete_cem_sampled_acs - self.omar_mu.unsqueeze(0)) ** 2, 0)) + 1e-5
-                    self.omar_sigma  = th.sqrt(th.mean((cem_sampled_acs - self.omar_mu.unsqueeze(0)) ** 2, 0)) + 1e-5
-
-                top_qvals, top_inds = th.topk(all_pred_qvals, 1, dim=0)
-                top_inds = top_inds # (1, bs, T-1, n_agents, 1)
-                top_acs = th.gather(discrete_cem_sampled_acs, dim=0, index=top_inds).squeeze(0).argmax(dim=-1) # (bs, T-1, n_agents)
+                #     # self.omar_mu = self._compute_softmax_acs(all_pred_qvals, discrete_cem_sampled_acs)
+                #     cem_sampled_acs = cem_sampled_acs.view(self.omar_num_samples, batch_size, -1, self.n_agents, self.n_actions)
+                #     self.omar_mu = self._compute_softmax_acs(all_pred_qvals, cem_sampled_acs)
+                #     # self.omar_sigma  = th.sqrt(th.mean((discrete_cem_sampled_acs - self.omar_mu.unsqueeze(0)) ** 2, 0)) + 1e-5
+                #     self.omar_sigma  = th.sqrt(th.mean((cem_sampled_acs - self.omar_mu.unsqueeze(0)) ** 2, 0)) + 1e-5
+
+                # top_qvals, top_inds = th.topk(all_pred_qvals, 1, dim=0)
+                # top_inds = top_inds # (1, bs, T-1, n_agents, 1)
+                # top_acs = th.gather(discrete_cem_sampled_acs, dim=0, index=top_inds).squeeze(0).argmax(dim=-1) # (bs, T-1, n_agents)
                 
-                pis = th.cat(pis, dim=1) # (bs, (T-1), n_agents, n_actions)
-                pis_mask = mask.expand_as(pis)
-                pis = pis.reshape(-1, self.n_actions)
-                omar_loss = F.cross_entropy(pis, top_acs.reshape(-1).detach(), reduction="mean")
-                omar_loss = omar_loss / (pis_mask.sum())
-
-                mask = mask.reshape(-1, 1)
-                td3_loss =  -(q * mask).mean() 
-
-                actor_loss = (1 - self.omar_coe) * td3_loss + self.omar_coe * omar_loss 
+                # get top_acs from q_values
+                # assert top_acs is not None
+                # pis = th.cat(pis, dim=1) # (bs, (T-1), n_agents, n_actions)
+                # pis_mask = mask.expand_as(pis)
+                # pis = pis.reshape(-1, self.n_actions)
+                # omar_loss = F.cross_entropy(pis, top_acs.reshape(-1).detach(), reduction="mean")
+                # omar_loss = omar_loss / (pis_mask.sum())
+
+                # mask = mask.reshape(-1, 1)
+                # td3_loss =  -(q * mask).mean() 
+
+                # actor_loss = (1 - self.omar_coe) * td3_loss + self.omar_coe * omar_loss 
             else: 
                 pis = th.cat(pis, dim=1) # (bs, (T-1), n_agents, n_actions)
                 pis[pis==-1e10] = 0
@@ -180,10 +182,10 @@ class ITD3Learner:
             if "bc" in self.args.name:
                 self.log_actor["bc_loss"].append(bc_loss.item())
                 self.log_actor["td3_loss"].append(td3_loss.item())
-            if "omar" in self.args.name:
-                raise NotImplementedError("Omar is not implemented yet")
-                self.log_actor["omar_loss"].append(omar_loss.item())
-                self.log_actor["td3_loss"].append(td3_loss.item())
+            # if "omar" in self.args.name:
+            #     #raise NotImplementedError("Omar is not implemented yet")
+            #     self.log_actor["omar_loss"].append(omar_loss.item())
+            #     self.log_actor["td3_loss"].append(td3_loss.item())
 
         self.training_steps += 1
         if t_env - self.log_stats_t >= self.args.learner_log_interval:
@@ -199,6 +201,7 @@ class ITD3Learner:
 
     def train_critic(self, batch: EpisodeBatch):
         critic_log = {}
+        top_acs = None
         batch_size = batch.batch_size
 
         rewards = batch["reward"][:, :-1] # (bs, T-1, 1)
@@ -259,36 +262,38 @@ class ITD3Learner:
         masked_td_error2  = td_error2 * mask.reshape(-1, 1)
         td_loss = 0.5 * (masked_td_error1 ** 2).mean() + 0.5 * (masked_td_error2 ** 2).mean()
 
-        if "cql" in self.args.name and getattr(self.args, "cql_type", "vanilla")=="vanilla":
-            # get over actions
-            assert self.args.critic_rnn is False
-            consq1 = []
-            consq2 = []
-            for i in range(self.n_actions):
-                consq1_i, consq2_i = [], []
-                self.critic1.init_hidden(batch.batch_size)
-                self.critic2.init_hidden(batch.batch_size)
-                tmp_action = th.zeros(batch.batch_size, self.n_agents, self.n_actions).to(batch.device)
-                tmp_action[:, :, i] = 1
-                for t in range(batch.max_seq_length-1):
-                    consq1_i.append(self.critic1(critic_inputs[:, t], tmp_action))
-                    consq2_i.append(self.critic2(critic_inputs[:, t], tmp_action))
-                consq1_i = th.stack(consq1_i, dim=1) # (bs, T-1, n_agents, 1)
-                consq2_i = th.stack(consq2_i, dim=1) # (bs, T-1, n_agents, 1)
-                consq1.append(consq1_i)
-                consq2.append(consq2_i)
-            consq1 = th.cat(consq1, dim=-1)
-            consq2 = th.cat(consq2, dim=-1)
-            consq1[avail_actions == 0] = 0
-            consq2[avail_actions == 0] = 0
-            cql_loss1 = ((th.logsumexp(consq1, dim=-1).reshape(-1, 1) - q_taken1.reshape(-1, 1)) * mask.reshape(-1, 1)).sum() / mask.sum()
-            cql_loss2 = ((th.logsumexp(consq2, dim=-1).reshape(-1, 1) - q_taken2.reshape(-1, 1)) * mask.reshape(-1, 1)).sum() / mask.sum()
-            cql_loss = (cql_loss1 + cql_loss2) / 2
-            critic_loss = self.args.cql_alpha * cql_loss + td_loss
-        elif "cql" in self.args.name:
-            raise NotImplementedError("not implenmeneted so far")
-        else:   
-            critic_loss = td_loss
+        # if ("cql" in self.args.name or "omar" in self.args.name) and getattr(self.args, "cql_type", "vanilla")=="vanilla":
+        #     # get over actions
+        #     assert self.args.critic_rnn is False
+        #     consq1 = []
+        #     consq2 = []
+        #     for i in range(self.n_actions):
+        #         consq1_i, consq2_i = [], []
+        #         self.critic1.init_hidden(batch.batch_size)
+        #         self.critic2.init_hidden(batch.batch_size)
+        #         tmp_action = th.zeros(batch.batch_size, self.n_agents, self.n_actions).to(batch.device)
+        #         tmp_action[:, :, i] = 1
+        #         for t in range(batch.max_seq_length-1):
+        #             consq1_i.append(self.critic1(critic_inputs[:, t], tmp_action))
+        #             consq2_i.append(self.critic2(critic_inputs[:, t], tmp_action))
+        #         consq1_i = th.stack(consq1_i, dim=1) # (bs, T-1, n_agents, 1)
+        #         consq2_i = th.stack(consq2_i, dim=1) # (bs, T-1, n_agents, 1)
+        #         consq1.append(consq1_i)
+        #         consq2.append(consq2_i)
+        #     consq1 = th.cat(consq1, dim=-1)
+        #     consq2 = th.cat(consq2, dim=-1)
+        #     consq1[avail_actions == 0] = 0
+        #     consq2[avail_actions == 0] = 0
+        #     #q_values = th.min(consq1, consq2) # (bs, T-1, n_agents, n_actions)
+        #     top_acs = consq1.argmax(dim=-1) # (bs, T-1, n_agents)
+        #     cql_loss1 = ((th.logsumexp(consq1, dim=-1).reshape(-1, 1) - q_taken1.reshape(-1, 1)) * mask.reshape(-1, 1)).sum() / mask.sum()
+        #     cql_loss2 = ((th.logsumexp(consq2, dim=-1).reshape(-1, 1) - q_taken2.reshape(-1, 1)) * mask.reshape(-1, 1)).sum() / mask.sum()
+        #     cql_loss = (cql_loss1 + cql_loss2) / 2
+        #     critic_loss = self.args.cql_alpha * cql_loss + td_loss
+        # elif "cql" in self.args.name:
+        #     raise NotImplementedError("not implenmeneted so far")
+        # else:   
+        critic_loss = td_loss
         self.critic_optimiser.zero_grad()
         critic_loss.backward()
         critic_grad_norm = th.nn.utils.clip_grad_norm_(self.critic_params, self.args.grad_norm_clip)
@@ -304,7 +309,7 @@ class ITD3Learner:
         critic_log["q_taken1_mean"] = (q_taken1).sum().item() / mask_elems
         critic_log["q_taken2_mean"] = (q_taken2).sum().item() / mask_elems
         critic_log["target_mean"] = targets.sum().item() / mask_elems
-        return critic_log
+        return critic_log, top_acs
         
     def _build_critic_inputs(self, batch, t=None):
         bs = batch.batch_size
diff --git a/src/learners/matd3_learner.py b/src/learners/matd3_learner.py
index 94df721..da5edef 100644
--- a/src/learners/matd3_learner.py
+++ b/src/learners/matd3_learner.py
@@ -48,10 +48,10 @@ class MATD3Learner:
             self.log_actor["bc_loss"] = []
             self.log_actor["td3_loss"] = []
 
-        if "cql" in self.args.name:
-            self.cql_alpha = self.args.cql_alpha
-            self.cql_temperature = self.args.cql_temperature
-            self.num_repeats = self.args.num_repeats
+        # if "cql" in self.args.name:
+        #     self.cql_alpha = self.args.cql_alpha
+        #     self.cql_temperature = self.args.cql_temperature
+        #     self.num_repeats = self.args.num_repeats
 
         device = "cuda" if args.use_cuda else "cpu"
         if self.args.standardise_returns:
@@ -209,71 +209,71 @@ class MATD3Learner:
         td_loss = 0.5 * (masked_td_error1 ** 2).mean() + 0.5 * (masked_td_error2 ** 2).mean()
 
 
-        if "cql" in self.args.name:
-            raise NotImplementedError("bad performance, to be improved")
-            # critic_inputs.shape = (bs, T-1, n_agents, x)
-            Tm1 = critic_inputs.shape[1]-1
-            formatted_critic_inputs = critic_inputs.unsqueeze(3).repeat(1, 1, 1, self.num_repeats, 1)
+        # if "cql" in self.args.name:
+        #     raise NotImplementedError("bad performance, to be improved")
+        #     # critic_inputs.shape = (bs, T-1, n_agents, x)
+        #     Tm1 = critic_inputs.shape[1]-1
+        #     formatted_critic_inputs = critic_inputs.unsqueeze(3).repeat(1, 1, 1, self.num_repeats, 1)
             
-            #### q_rand (bs*(T-1)*n_agents, num_repeats 1)
-            random_actions = F.one_hot(th.randint(low=0, high=self.n_actions, size=(batch_size, Tm1, self.num_repeats, self.n_agents), device=batch.device))
-            random_actions = random_actions.view(batch_size, Tm1, self.num_repeats, 1, self.n_agents*self.n_actions).expand(-1, -1, -1, self.n_agents, -1)
-            random_actions = random_actions.transpose(2, 3)
-            #random_log_prob = np.log(1 / self.n_actions * self.n_actions) # = 0
-            random_Q1 = self.critic1(formatted_critic_inputs[:, :-1], random_actions) #- random_log_prob
-            random_Q2 = self.critic2(formatted_critic_inputs[:, :-1], random_actions) #- random_log_prob
-            random_Q1 = random_Q1.reshape(-1, self.num_repeats, 1)
-            random_Q2 = random_Q2.reshape(-1, self.num_repeats, 1)
+        #     #### q_rand (bs*(T-1)*n_agents, num_repeats 1)
+        #     random_actions = F.one_hot(th.randint(low=0, high=self.n_actions, size=(batch_size, Tm1, self.num_repeats, self.n_agents), device=batch.device))
+        #     random_actions = random_actions.view(batch_size, Tm1, self.num_repeats, 1, self.n_agents*self.n_actions).expand(-1, -1, -1, self.n_agents, -1)
+        #     random_actions = random_actions.transpose(2, 3)
+        #     #random_log_prob = np.log(1 / self.n_actions * self.n_actions) # = 0
+        #     random_Q1 = self.critic1(formatted_critic_inputs[:, :-1], random_actions) #- random_log_prob
+        #     random_Q2 = self.critic2(formatted_critic_inputs[:, :-1], random_actions) #- random_log_prob
+        #     random_Q1 = random_Q1.reshape(-1, self.num_repeats, 1)
+        #     random_Q2 = random_Q2.reshape(-1, self.num_repeats, 1)
                 
             
-            #### q_cur
-
-            cur_actions, cur_log_probs = [], []
-            with th.no_grad():
-                self.mac.init_hidden(batch_size, self.num_repeats)
-                for t in range(batch.max_seq_length-1):
-                    cur_action, cur_log_prob = self.mac.get_repeat_actions(batch, t, self.num_repeats)
-                    cur_actions.append(cur_action)
-                    cur_log_probs.append(cur_log_prob)
-                cur_actions = th.stack(cur_actions, dim=1) # (bs, T-1, n_agents, num_repeats, n_actions)
-                cur_log_probs = th.stack(cur_log_probs, dim=1).unsqueeze(-1) # (bs, T-1, n_agents, num_repeats, 1)
-
-            cur_actions = cur_actions.transpose(2, 3).contiguous().view(batch_size, Tm1, self.num_repeats, 1, self.n_agents*self.n_actions)
-            cur_actions = cur_actions.expand(-1, -1, -1, self.n_agents, -1).transpose(2, 3)
-            cur_Q1 = self.critic1(formatted_critic_inputs[:, :-1], cur_actions.detach())# - cur_log_probs.detach()
-            cur_Q2 = self.critic2(formatted_critic_inputs[:, :-1], cur_actions.detach())# - cur_log_probs.detach()
-            cur_Q1 = cur_Q1.reshape(-1, self.num_repeats, 1)
-            cur_Q2 = cur_Q2.reshape(-1, self.num_repeats, 1)
+        #     #### q_cur
+
+        #     cur_actions, cur_log_probs = [], []
+        #     with th.no_grad():
+        #         self.mac.init_hidden(batch_size, self.num_repeats)
+        #         for t in range(batch.max_seq_length-1):
+        #             cur_action, cur_log_prob = self.mac.get_repeat_actions(batch, t, self.num_repeats)
+        #             cur_actions.append(cur_action)
+        #             cur_log_probs.append(cur_log_prob)
+        #         cur_actions = th.stack(cur_actions, dim=1) # (bs, T-1, n_agents, num_repeats, n_actions)
+        #         cur_log_probs = th.stack(cur_log_probs, dim=1).unsqueeze(-1) # (bs, T-1, n_agents, num_repeats, 1)
+
+        #     cur_actions = cur_actions.transpose(2, 3).contiguous().view(batch_size, Tm1, self.num_repeats, 1, self.n_agents*self.n_actions)
+        #     cur_actions = cur_actions.expand(-1, -1, -1, self.n_agents, -1).transpose(2, 3)
+        #     cur_Q1 = self.critic1(formatted_critic_inputs[:, :-1], cur_actions.detach())# - cur_log_probs.detach()
+        #     cur_Q2 = self.critic2(formatted_critic_inputs[:, :-1], cur_actions.detach())# - cur_log_probs.detach()
+        #     cur_Q1 = cur_Q1.reshape(-1, self.num_repeats, 1)
+        #     cur_Q2 = cur_Q2.reshape(-1, self.num_repeats, 1)
             
-            #### q_nxt
-            nxt_actions, nxt_log_probs = [], []
-            with th.no_grad():
-                self.mac.init_hidden(batch_size, self.num_repeats)
-                for t in range(batch.max_seq_length):
-                    nxt_action, nxt_log_prob = self.mac.get_repeat_actions(batch, t, self.num_repeats)
-                    nxt_actions.append(nxt_action)
-                    nxt_log_probs.append(nxt_log_prob)
-                nxt_actions = th.stack(nxt_actions, dim=1)[:, 1:] # (bs, T-1, n_agents, num_repeats, n_actions)
-                nxt_log_probs = th.stack(nxt_log_probs, dim=1).unsqueeze(-1)[:, 1:] # (bs, T-1, n_agents, num_repeats, 1)
-            nxt_actions = nxt_actions.transpose(2, 3).contiguous().view(batch_size, Tm1, self.num_repeats, 1, self.n_agents*self.n_actions)
-            nxt_actions = nxt_actions.expand(-1, -1, -1, self.n_agents, -1).transpose(2, 3)
-            nxt_Q1 = self.critic1(formatted_critic_inputs[:, 1:], nxt_actions.detach())# - nxt_log_probs.detach()
-            nxt_Q2 = self.critic2(formatted_critic_inputs[:, 1:], nxt_actions.detach())# - nxt_log_probs.detach()
-            nxt_Q1 = nxt_Q1.reshape(-1, self.num_repeats, 1)
-            nxt_Q2 = nxt_Q2.reshape(-1, self.num_repeats, 1)
+        #     #### q_nxt
+        #     nxt_actions, nxt_log_probs = [], []
+        #     with th.no_grad():
+        #         self.mac.init_hidden(batch_size, self.num_repeats)
+        #         for t in range(batch.max_seq_length):
+        #             nxt_action, nxt_log_prob = self.mac.get_repeat_actions(batch, t, self.num_repeats)
+        #             nxt_actions.append(nxt_action)
+        #             nxt_log_probs.append(nxt_log_prob)
+        #         nxt_actions = th.stack(nxt_actions, dim=1)[:, 1:] # (bs, T-1, n_agents, num_repeats, n_actions)
+        #         nxt_log_probs = th.stack(nxt_log_probs, dim=1).unsqueeze(-1)[:, 1:] # (bs, T-1, n_agents, num_repeats, 1)
+        #     nxt_actions = nxt_actions.transpose(2, 3).contiguous().view(batch_size, Tm1, self.num_repeats, 1, self.n_agents*self.n_actions)
+        #     nxt_actions = nxt_actions.expand(-1, -1, -1, self.n_agents, -1).transpose(2, 3)
+        #     nxt_Q1 = self.critic1(formatted_critic_inputs[:, 1:], nxt_actions.detach())# - nxt_log_probs.detach()
+        #     nxt_Q2 = self.critic2(formatted_critic_inputs[:, 1:], nxt_actions.detach())# - nxt_log_probs.detach()
+        #     nxt_Q1 = nxt_Q1.reshape(-1, self.num_repeats, 1)
+        #     nxt_Q2 = nxt_Q2.reshape(-1, self.num_repeats, 1)
             
-            cat_Q1 = th.cat([random_Q1, cur_Q1, nxt_Q1], dim=1)
-            cat_Q2 = th.cat([random_Q2, cur_Q2, nxt_Q2], dim=1)
-            cat_q1_vals = th.logsumexp(cat_Q1 / self.cql_temperature, dim=1) * self.cql_temperature
-            cat_q2_vals = th.logsumexp(cat_Q2 / self.cql_temperature, dim=1) * self.cql_temperature
-            masked_cql_loss1 = ((cat_q1_vals-q_taken1.reshape(-1, 1)) * mask.reshape(-1, 1)).sum() / mask.sum()
-            masked_cql_loss2 = ((cat_q2_vals-q_taken2.reshape(-1, 1)) * mask.reshape(-1, 1)).sum() / mask.sum()
+        #     cat_Q1 = th.cat([random_Q1, cur_Q1, nxt_Q1], dim=1)
+        #     cat_Q2 = th.cat([random_Q2, cur_Q2, nxt_Q2], dim=1)
+        #     cat_q1_vals = th.logsumexp(cat_Q1 / self.cql_temperature, dim=1) * self.cql_temperature
+        #     cat_q2_vals = th.logsumexp(cat_Q2 / self.cql_temperature, dim=1) * self.cql_temperature
+        #     masked_cql_loss1 = ((cat_q1_vals-q_taken1.reshape(-1, 1)) * mask.reshape(-1, 1)).sum() / mask.sum()
+        #     masked_cql_loss2 = ((cat_q2_vals-q_taken2.reshape(-1, 1)) * mask.reshape(-1, 1)).sum() / mask.sum()
 
-            cql_loss = (masked_cql_loss1 + masked_cql_loss2) / 2
+        #     cql_loss = (masked_cql_loss1 + masked_cql_loss2) / 2
 
-            critic_loss = td_loss + self.cql_alpha * cql_loss
-        else:
-            critic_loss = td_loss
+        #     critic_loss = td_loss + self.cql_alpha * cql_loss
+        # else:
+        critic_loss = td_loss
 
         self.critic_optimiser.zero_grad()
         critic_loss.backward()
@@ -283,10 +283,10 @@ class MATD3Learner:
         mask_elems = mask.sum().item()
         critic_log["critic_loss"] = critic_loss.item()
         critic_log["critic_grad_norm"] = critic_grad_norm.item()
-        if "cql" in self.args.name:
-            raise NotImplementedError()
-            critic_log["td_loss"] = td_loss.item()
-            critic_log["cql_loss"] = cql_loss.item()
+        # if "cql" in self.args.name:
+        #     raise NotImplementedError()
+        #     critic_log["td_loss"] = td_loss.item()
+        #     critic_log["cql_loss"] = cql_loss.item()
 
         critic_log["td_error1_abs"] = masked_td_error1.abs().sum().item() / mask_elems
         critic_log["td_error2_abs"] = masked_td_error2.abs().sum().item() / mask_elems
diff --git a/src/main.py b/src/main.py
index 55f2dc2..1f87ee4 100644
--- a/src/main.py
+++ b/src/main.py
@@ -196,7 +196,10 @@ if __name__ == '__main__':
     else:
         config_dict['remark'] = ''
 
-    unique_token = "seed_{}_{}{}_{}".format(config_dict['seed'], config_dict['name'], config_dict['remark'], datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S"))
+    # result_dir_format: {run_file}/{Y_M_D}/.../{remark}/{seed}_{HMS}
+    Y_M_D, H_M_S = datetime.datetime.now().strftime("%Y_%m_%d %H_%M_%S").split()
+    unique_token = "seed_{}_{}".format(config_dict['seed'], H_M_S)
+
     config_dict['unique_token'] = unique_token
 
     match config_dict["env"]:
@@ -213,7 +216,7 @@ if __name__ == '__main__':
                 results_path = os.path.join(results_path, 'evaluate')
             
             results_save_dir = os.path.join(
-                results_path, "offline", env, map_name,
+                results_path, "offline", Y_M_D, env, map_name,
                 config_dict['offline_data_quality'],
                 config_dict['name'] + config_dict['remark'],
                 unique_token
@@ -224,7 +227,7 @@ if __name__ == '__main__':
             else:
                 aux_dir = f"stop_return_{config_dict['stop_return']}"
             results_save_dir = os.path.join(
-                results_path, "collect", 
+                results_path, "collect", Y_M_D,
                 env + os.sep + map_name,
                 config_dict['offline_data_quality'], aux_dir,
                 config_dict['name'] + config_dict['remark'],
@@ -235,7 +238,7 @@ if __name__ == '__main__':
                 results_path = os.path.join(results_path, 'evaluate')
                                         
             results_save_dir = os.path.join(
-                results_path, "mto", env, config_dict['task'],
+                results_path, "mto", Y_M_D, env, config_dict['task'],
                 '+'.join([f'{k}-{v}' for k, v in config_dict['train_tasks_data_quality'].items()]),
                 config_dict['name'] + config_dict['remark'],
                 unique_token
@@ -244,7 +247,7 @@ if __name__ == '__main__':
             if config_dict['evaluate']:
                 results_path = os.path.join(results_path, 'evaluate')
             results_save_dir = os.path.join(
-                results_path, "run_online", 
+                results_path, "run_online", Y_M_D,
                 env + os.sep + map_name, 
                 config_dict['name'] + config_dict['remark'],
                 unique_token
diff --git a/src/offline_run.py b/src/offline_run.py
index 3e5c9fc..439dec5 100644
--- a/src/offline_run.py
+++ b/src/offline_run.py
@@ -34,17 +34,27 @@ def run(_run, _config, _log):
     _log.info("\n\n" + experiment_params + "\n")
 
     results_save_dir = args.results_save_dir
+    if args.use_wandb:
+        args.use_tensorboard = False
+    # assert args.use_tensorboard and args.use_wandb
+    
     
     if args.use_tensorboard and not args.evaluate:
         # only log tensorboard when in training mode
-        tb_exp_direc = os.path.join(results_save_dir, 'tb_logs')
+        tb_exp_direc = os.path.join(results_save_dir, 'logs')
         logger.setup_tb(tb_exp_direc)
         
         # write config file
         config_str = json.dumps(vars(args), indent=4)
         with open(os.path.join(results_save_dir, "config.json"), "w") as f:
             f.write(config_str)
-
+    
+    if args.use_wandb and not args.evaluate:
+        wandb_run_name = args.results_save_dir.split('/')
+        wandb_run_name = "/".join(wandb_run_name[wandb_run_name.index("results")+1:])
+        wandb_exp_direc = os.path.join(results_save_dir, 'logs')
+        logger.setup_wandb(wandb_exp_direc, project=args.wandb_project_name, name=wandb_run_name,
+                           run_id=args.resume_id, config=args)
     # set model save dir
     args.save_dir = os.path.join(results_save_dir, 'models')
 
@@ -241,7 +251,7 @@ def train_sequential(args, logger, learner, runner, offline_buffer):
             logger.log_stat("episode", episode, t_env)
             logger.print_recent_stats()
 
-    logger.console_logger.info("Finsh training sequential")
+    logger.console_logger.info("Finish training sequential")
             
 
 
diff --git a/src/runners/parallel_runner.py b/src/runners/parallel_runner.py
index c644cbf..383545c 100644
--- a/src/runners/parallel_runner.py
+++ b/src/runners/parallel_runner.py
@@ -87,7 +87,7 @@ class ParallelRunner:
         self.t = 0
         self.env_steps_this_run = 0
 
-    def run(self, test_mode=False, pretrain_phase=False, evaluate_mode=False):
+    def run(self, test_mode=False, evaluate_mode=False):
         self.reset()
 
         all_terminated = False
@@ -102,12 +102,8 @@ class ParallelRunner:
 
             # Pass the entire batch of experiences up till now to the agents
             # Receive the actions for each agent at this timestep in a batch for each un-terminated env
-            if pretrain_phase:
-                # If pretrain phase, just select action randomly
-                # t_env is used for epsilon_greedyr
-                actions = self.mac.select_actions(self.batch, t_ep=self.t, t_env=0, bs=envs_not_terminated, test_mode=False)
-            else:
-                actions = self.mac.select_actions(self.batch, t_ep=self.t, t_env=self.t_env, bs=envs_not_terminated, test_mode=test_mode)
+            
+            actions = self.mac.select_actions(self.batch, t_ep=self.t, t_env=self.t_env, bs=envs_not_terminated, test_mode=test_mode)
             
             cpu_actions = actions.to("cpu").numpy()
 
@@ -189,30 +185,30 @@ class ParallelRunner:
             env_stat = parent_conn.recv()
             env_stats.append(env_stat)
 
-        if not pretrain_phase:  
-            cur_stats = self.test_stats if test_mode else self.train_stats
-            cur_returns = self.test_returns if test_mode else self.train_returns
-            log_prefix = "test_" if test_mode else ""
-            infos = [cur_stats] + final_env_infos
-            cur_stats.update({k: sum(d.get(k, 0) for d in infos) for k in set.union(*[set(d) for d in infos])})
-            cur_stats["n_episodes"] = self.batch_size + cur_stats.get("n_episodes", 0)
-            cur_stats["ep_length"] = sum(episode_lengths) + cur_stats.get("ep_length", 0)
-
-            cur_returns.extend(episode_returns)
-
-            n_test_runs = max(1, self.args.test_nepisode // self.batch_size) * self.batch_size
-            if not evaluate_mode and test_mode and (len(self.test_returns) == n_test_runs):
-                self._log(cur_returns, cur_stats, log_prefix)
-            elif evaluate_mode and test_mode and len(self.test_returns) == n_test_runs:
-                print(f"========== test result ==========")
-                print(f"return_mean: {np.mean(self.test_returns)}")
-                print(f"return_std: {np.std(self.test_returns)}")
-                return self.test_stats, self.test_returns
-            elif not evaluate_mode and self.t_env - self.log_train_stats_t >= self.args.runner_log_interval:
-                self._log(cur_returns, cur_stats, log_prefix)
-                if hasattr(self.mac.action_selector, "epsilon"):
-                    self.logger.log_stat("epsilon", self.mac.action_selector.epsilon, self.t_env)
-                self.log_train_stats_t = self.t_env
+        
+        cur_stats = self.test_stats if test_mode else self.train_stats
+        cur_returns = self.test_returns if test_mode else self.train_returns
+        log_prefix = "test_" if test_mode else ""
+        infos = [cur_stats] + final_env_infos
+        cur_stats.update({k: sum(d.get(k, 0) for d in infos) for k in set.union(*[set(d) for d in infos])})
+        cur_stats["n_episodes"] = self.batch_size + cur_stats.get("n_episodes", 0)
+        cur_stats["ep_length"] = sum(episode_lengths) + cur_stats.get("ep_length", 0)
+
+        cur_returns.extend(episode_returns)
+
+        n_test_runs = max(1, self.args.test_nepisode // self.batch_size) * self.batch_size
+        if not evaluate_mode and test_mode and (len(self.test_returns) == n_test_runs):
+            self._log(cur_returns, cur_stats, log_prefix)
+        elif evaluate_mode and test_mode and len(self.test_returns) == n_test_runs:
+            print(f"========== test result ==========")
+            print(f"return_mean: {np.mean(self.test_returns)}")
+            print(f"return_std: {np.std(self.test_returns)}")
+            return self.test_stats, self.test_returns
+        elif not evaluate_mode and self.t_env - self.log_train_stats_t >= self.args.runner_log_interval:
+            self._log(cur_returns, cur_stats, log_prefix)
+            if hasattr(self.mac.action_selector, "epsilon"):
+                self.logger.log_stat("epsilon", self.mac.action_selector.epsilon, self.t_env)
+            self.log_train_stats_t = self.t_env
 
         return self.batch
 
diff --git a/src/utils/logging.py b/src/utils/logging.py
index 7106db7..1faeac7 100644
--- a/src/utils/logging.py
+++ b/src/utils/logging.py
@@ -8,6 +8,7 @@ class Logger:
         self.console_logger = console_logger
 
         self.use_tb = False
+        self.use_wandb = False
         self.use_sacred = False
         self.use_hdf = False
 
@@ -15,10 +16,33 @@ class Logger:
 
     def setup_tb(self, directory_name):
         # Import here so it doesn't have to be installed if you don't use it
-        from tensorboardX import SummaryWriter
         self.writer = SummaryWriter(logdir=directory_name)
         self.use_tb = True
 
+    def setup_wandb(self, directory_name, project=None, name=None, run_id=None, entity=None, config=None):
+        self.use_wandb = True
+    
+        import wandb
+        self.wandb_run = (
+            wandb.init(
+                project=project,
+                name=name,
+                id=run_id,
+                resume="allow",
+                entity=entity,
+                sync_tensorboard=True,
+                config=config,  # type: ignore
+            )
+            if not wandb.run
+            else wandb.run
+        )
+        self.wandb_run._label(repo="offpymarl")  # type: ignore
+        self.writer = SummaryWriter(logdir=directory_name)
+        self.writer.add_text(
+        "hyperparameters",
+        "|param|value|\n|-|-|\n%s" % ("\n".join([f"|{key}|{value}|" for key, value in vars(config).items()])),
+    )
+        
     def setup_sacred(self, sacred_run_dict):
         self.sacred_info = sacred_run_dict.info
         self.use_sacred = True
@@ -26,7 +50,7 @@ class Logger:
     def log_stat(self, key, value, t, to_sacred=True):
         self.stats[key].append((t, value))
 
-        if self.use_tb:
+        if self.use_tb or self.use_wandb:
             self.writer.add_scalar(key, value, t)
 
         if self.use_sacred and to_sacred:
